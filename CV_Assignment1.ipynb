{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhs42/DeepLearnCVBootCamp/blob/main/CV_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nwRbwMoPrdX"
      },
      "source": [
        "# **Computer Vision Track - Assignment 1**\n",
        "## **Artificial Neural Network for Image Classification**\n",
        "\n",
        "In this assignment, we explore the fundamentals of building a neural network for image classification. Image classification is a core task in computer vision, where the goal is to correctly identify and categorize objects in images. We will design and implement a neural network from scratch using popular machine learning libraries, applying key concepts such as activation functions, and backpropagation. By training the model on a labeled dataset, we aim to evaluate its performance and understand how different network architectures influence classification accuracy. This assignment serves as a practical introduction to deep learning in computer vision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9kAubNDPrda"
      },
      "source": [
        "===================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9HNCvuLPrdb"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 1**\n",
        "\n",
        "The code below imports essential libraries for building and training an artificial neural network (ANN) using PyTorch.\n",
        "1) <blue>**torch**</blue> and <blue>**torch.nn**</blue> provide core functionality for tensors and neural networks.\n",
        "2) <blue>**optim**</blue> offers optimization algorithms (like <green>**Adam**</green>).\n",
        "3) <blue>**torchvision**</blue> helps with image-related tasks, and transforms aids in image preprocessing.\n",
        "4) <blue>**DataLoader**</blue> is used to load datasets in batches.\n",
        "5) Libraries like <blue>**matplotlib**</blue>, <blue>**seaborn**</blue>, and <blue>**sklearn.metrics**</blue> help visualize results and evaluate the model using metrics such as <green>**accuracy**</green>, <green>**precision**</green>, <green>**recall**</green> etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zlGs4hZxPrdb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgo04L_Prdc"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 2**\n",
        "1) The <blue>**device**</blue> variable ensures the model runs on a <green>**GPU**</green> if available; otherwise, it falls back to a <green>**CPU**</green>.\n",
        "2) The <blue>**transform**</blue> object defines the <green>**preprocessing**</green> steps for images\n",
        "3) <blue>**ToTensor()**</blue> converts images to tensors.\n",
        "4) <blue>**Normalize((0.5,), (0.5,))**</blue> normalizes the images to a range between -1 and 1, helping with better convergence during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EamGEHklPrdd"
      },
      "outputs": [],
      "source": [
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations (Normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the dataset to range [-1, 1]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3X2Bh7IPrdd"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 3**\n",
        "1) This code outlines loading the <blue>**MNIST**</blue> dataset for <green>**handwritten digit recognition**</green>.\n",
        "2) The <blue>**train_loader**</blue> and <blue>**test_loader**</blue> are used to load the datasets in batches of 64, with shuffling applied only to the training set.\n",
        "3) The <blue>**dataiter**</blue> and next commands allow viewing a batch of images and labels, which can be useful for visualization before training.\n",
        "\n",
        "\n",
        "Note: The placeholders **\"None\"** need to be replaced with the correct dataset-loading code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2t7Xa7bPrde"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "# TODO: Download and load the training and test sets using torchvision.datasets\n",
        "train_dataset = None  # Replace None with the correct code\n",
        "test_dataset = None  # Replace None with the correct code\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Visualize some images (Optional)\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E7sozDDPrde"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 4**\n",
        "1) This code defines a simple <green>**feedforward artificial neural network (ANN)**</green> for classifying MNIST digits.\n",
        "2) The model has three fully connected layers <blue>**(fc1, fc2, fc3)**</blue>, and the final layer outputs predictions for <green>**10 classes (digits 0–9)**</green>.\n",
        "3) The <green>**forward pass**</green> describes how the input data flows through the network\n",
        "4) The input image is first <green>**flattened**</green>.\n",
        "5) It passes through fully connected layers with <blue>**ReLU activation function**</blue>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate numbers of units and activation functions in each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKvY2HfCPrdf"
      },
      "outputs": [],
      "source": [
        "# Define the neural network model\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        # TODO: Define the layers (Flatten, Fully Connected, Activation functions)\n",
        "        self.fc1 = nn.Linear(28*28, None)  # Replace None with the correct number of units\n",
        "        self.fc2 = nn.Linear(None, None)   # Replace None with the correct number of units\n",
        "        self.fc3 = nn.Linear(None, 10)     # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass\n",
        "        x = x.view(-1, 28*28)  # Flatten the image\n",
        "        x = None  # First fully connected layer + activation\n",
        "        x = None  # Second fully connected layer + activation\n",
        "        x = None  # Output layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkQ9l65mPrdf"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 5**\n",
        "1) The <blue>**ANN model**</blue> is instantiated and moved to the <green>**selected device (CPU or GPU)**</green>.\n",
        "2) The loss function is set to <blue>**CrossEntropyLoss**</blue>, which is suitable for <green>**multi-class classification**</green> problems like MNIST.\n",
        "3) The optimizer is <blue>**Adam**</blue>, with a <blue>**learning rate**</blue> of <green>**0.001**</green>, used to adjust the model parameters during training based on gradients computed from the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6aIeOh4Prdf"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = ANN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuJXZw4qPrdg"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 6**\n",
        "This code trains the neural network over a specified number of epochs (num_epochs).\n",
        "For each batch of images and labels, the following steps are performed:\n",
        "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU or GPU)**</green>.\n",
        "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute the <green>**output predictions**</green>.\n",
        "3) <blue>**Loss Calculation**</blue>: The loss between the predictions and true labels is computed and added to <blue>**ls_losses**</blue> for tracking.\n",
        "4) <blue>**Backpropagation and Optimization**</blue>: Gradients are calculated using backpropagation, and the optimizer <green>**updates the model parameters**</green> based on these gradients.\n",
        "\n",
        "Every 100 batches, the loss is printed to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW7YoMBlPrdg"
      },
      "outputs": [],
      "source": [
        "# Training the network\n",
        "num_epochs = None # Choose the appropriate number of epochs\n",
        "ls_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = None  # Replace None with code to move images to device and flatten\n",
        "        labels = None  # Move labels to device\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = None  # Forward pass through the model\n",
        "        loss = None     # Compute the loss\n",
        "        ls_losses.append(loss.detach().numpy())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        None  # Backward pass\n",
        "        None  # Optimize the weights\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43LoiNKgPrdg"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 7**\n",
        "1) This code plots the <blue>**training losses**</blue> that were recorded in <blue>**ls_losses**</blue> during training.\n",
        "2) The <blue>**x-axis**</blue> represents the <green>**number of samples (batches)**</green> seen during training, and the <blue>**y-axis**</blue> shows the <green>**corresponding loss**</green>.\n",
        "3) It provides a visual representation of how the <green>**model's loss decreases over time**</green>, indicating whether the training is progressing well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f_wNHxqPrdg"
      },
      "outputs": [],
      "source": [
        "# Plot Losses\n",
        "x_axis = np.arange(0, len(ls_losses), 1)\n",
        "plt.plot(x_axis, ls_losses)\n",
        "plt.xlabel = \"Sample\"\n",
        "plt.ylabel = \"Loss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QyuPxmPrdg"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 8**\n",
        "This block evaluates the model on the test dataset.\\\n",
        "<blue>**model.eval()**</blue> sets the model to <green>**evaluation mode**</green>, disabling dropout layers and stopping the computation of gradients to save memory and speed up computations.\n",
        "For each batch of test images:\n",
        "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU/GPU)**</green>.\n",
        "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute <green>**predictions**</green>.\n",
        "3) <blue>**Accuracy Calculation**</blue>: The model's predictions are compared to the true labels, and the <green>**accuracy**</green> is calculated.\n",
        "4) <blue>**Storing Results**</blue>: Predictions and true labels are saved to <green>**calculate metrics**</green> later.\n",
        "\n",
        "At the end of testing, the accuracy of the model on the test dataset is printed.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code for data movement, prediction, and storing results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgmdsezIPrdg"
      },
      "outputs": [],
      "source": [
        "# Testing the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = None  # Replace None with code to move images to device and flatten\n",
        "        labels = None  # Move labels to device\n",
        "\n",
        "        outputs = None  # Forward pass through the model\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # TODO: Append predictions and labels for metrics\n",
        "        all_preds.extend(None)  # Append predictions to the list\n",
        "        all_labels.extend(None)  # Append actual labels to the list\n",
        "\n",
        "    print(f'Accuracy of the network on the test images: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR5xisHdPrdh"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 9**\n",
        "This code calculates key performance metrics to evaluate the model:\n",
        "1) <blue>**Accuracy**</blue>: Percentage of correctly classified samples.\n",
        "2) <blue>**Precision**</blue>: Proportion of true positive predictions out of all positive predictions.\n",
        "3) <blue>**Recall**</blue>: Proportion of true positives out of actual positive samples.\n",
        "4) <blue>**F1-Score**</blue>: <green>**Harmonic mean**</green> of precision and recall.\n",
        "5) The <blue>**confusion matrix**</blue> is also calculated, showing the number of correct and incorrect predictions for each class. It is visualized using a <green>**heatmap**</green>, where the <blue>**rows**</blue> represent <green>**true labels**</green>, and the <blue>**columns**</blue> represent <green>**predicted labels**</green>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKBdtT7ePrdh"
      },
      "outputs": [],
      "source": [
        "# TODO: Convert predictions and labels to numpy arrays for metric calculation\n",
        "all_preds = None  # Replace None with correct code to convert to numpy array\n",
        "all_labels = None  # Replace None with correct code to convert to numpy array\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1-score\n",
        "# TODO: Use sklearn to calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = None  # Replace None with the correct calculation\n",
        "precision = None  # Replace None with the correct calculation\n",
        "recall = None  # Replace None with the correct calculation\n",
        "f1 = None  # Replace None with the correct calculation\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "\n",
        "# TODO: Confusion matrix calculation\n",
        "conf_matrix = None  # Replace None with correct code to compute the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc5cf76e"
      },
      "source": [
        "### **Step 10 (For Advanced Level, optional for Basic Level)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dee7b24e"
      },
      "source": [
        "Hyperparameter tuning is a crucial step in optimizing the performance of a neural network. It involves finding the best set of hyperparameters that control the learning process and the model's architecture, such as the learning rate, batch size, number of epochs, and the number of units in each layer.\n",
        "\n",
        "Techniques like grid search or random search can be employed to systematically explore different combinations of these hyperparameters and identify the values that yield the best results on a validation set. This process helps in improving the model's accuracy and generalization capabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6a32469"
      },
      "source": [
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'learning_rate': None, # Choose appropriate range\n",
        "    'batch_size': None, # Choose appropriate range\n",
        "    'num_epochs': None # Choose appropriate range\n",
        "}\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Iterate through hyperparameter combinations\n",
        "for lr in param_grid['learning_rate']:\n",
        "    for bs in param_grid['batch_size']:\n",
        "        for epochs in param_grid['num_epochs']:\n",
        "            print(f\"Training with LR: {lr}, Batch Size: {bs}, Epochs: {epochs}\")\n",
        "\n",
        "            # Re-initialize model, loss, and optimizer\n",
        "            model = ANN().to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Update DataLoader with new batch size\n",
        "            train_loader = DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "            test_loader = DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            # Training loop (similar to Step 6)\n",
        "            for epoch in range(epochs):\n",
        "                for i, (images, labels) in enumerate(train_loader):\n",
        "                    images = images.to(device).view(-1, 28*28)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluation loop (similar to Step 8)\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for images, labels in test_loader:\n",
        "                    images = images.to(device).view(-1, 28*28)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                accuracy = None # Write the correct formula\n",
        "                print(f'Accuracy for LR: {lr}, Batch Size: {bs}, Epochs: {epochs}: {accuracy:.2f}%')\n",
        "\n",
        "                # Store best parameters\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = {'learning_rate': lr, 'batch_size': bs, 'num_epochs': epochs}\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(best_params)\n",
        "print(f\"Best Accuracy: {best_accuracy:.2f}%\")\n",
        "# Optional: Add a code cell to visualise the results with different parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64e1512"
      },
      "source": [
        "### **Step 11 (For Advanced Level, optional for Basic Level)**\n",
        "\n",
        "Regularization is a set of techniques used to prevent overfitting in neural networks. Overfitting occurs when a model learns the training data too well, including the noise and outliers, which negatively impacts its performance on unseen data. Regularization methods add a penalty to the loss function or modify the network architecture to discourage the model from becoming too complex. Common regularization techniques include L1 and L2 regularization, and dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70740bfc"
      },
      "source": [
        "# Add regularization (Dropout) to the ANN model\n",
        "class ANN_Regularized(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN_Regularized, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, None)  # Replace with number of units in first hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(None)  # Replace with dropout probability\n",
        "        self.fc2 = nn.Linear(None, None)  # Replace input units (same as previous layer) → next hidden layer units\n",
        "        self.fc3 = nn.Linear(None, None)  # Replace second hidden layer units → number of output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, None)  # Replace with flattened size\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the regularized model\n",
        "model_regularized = None  # Replace with the correct model class name\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_regularized = None  # Replace with the appropriate loss function\n",
        "optimizer_regularized = None  # Replace with optimizer initialization\n",
        "\n",
        "print(\"Regularized ANN model defined with dropout layers.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}