{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhs42/DeepLearnCVBootCamp/blob/main/NLP_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing Assignment 1**"
      ],
      "metadata": {
        "id": "7s63z2d2d5ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Objective** - You have to implement all of these techniques on a small corpus of your own choice taken from the web. Make sure to use the same corpus for all the tasks, also please give the reference of where you chose the corpus from. People attempting for advanced level are recommended to use a larger(production level/real-use) dataset."
      ],
      "metadata": {
        "id": "FC1_6Nu9ikum"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyTOfj8wYEj8"
      },
      "source": [
        "\n",
        "## **Text Pre-processing for Natural Language Processing (NLP)**\n",
        "\n",
        "Text preprocessing in NLP is the process of cleaning and preparing raw text data for analysis by machine learning or deep learning models. It involves converting text into a structured format through various steps. These steps ensure the text is consistent and meaningful, enabling models to better understand and process the data efficiently. Proper preprocessing is essential for improving the performance and accuracy of NLP models. The steps involved in pre-processing which we are going to be looking at in this tutorial are:\n",
        "1) **Lowercasing**\n",
        "2) **Removing Punctuations and Special Characrters**\n",
        "3) **Stop-Words removal**\n",
        "4) **Removal of URLs**\n",
        "5) **Removal of HTML Tags**\n",
        "6) **Stemming**\n",
        "7) **Lemmatization**\n",
        "8) **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQ_tbI1YEkH"
      },
      "source": [
        "===================================================================================================="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and set up Kaggle API\n",
        "\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "#will work only if kaggle.json is already in the current directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "metadata": {
        "id": "XjEQRCROm7CD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download E-commerce Reviews dataset into pandas\n",
        "\n",
        "kaggle_url = \"https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews\"\n",
        "\n",
        "import re\n",
        "match = re.match(r\".*kaggle\\.com/datasets/([^/]+)/([^/?#]+)\", kaggle_url)\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "owner, dataset = match.group(1), match.group(2)\n",
        "!kaggle datasets download -d {owner}/{dataset} --unzip -p ./dataset\n",
        "\n",
        "csv_files = [f for f in os.listdir(\"./dataset\") if f.endswith(\".csv\")]\n",
        "df = pd.read_csv(os.path.join(\"./dataset\", csv_files[0]))\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFmiXAvhontT",
        "outputId": "890e2b0d-44ce-4a0a-de88-8970cdd96c10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading womens-ecommerce-clothing-reviews.zip to ./dataset\n",
            "  0% 0.00/2.79M [00:00<?, ?B/s]\n",
            "100% 2.79M/2.79M [00:00<00:00, 570MB/s]\n",
            "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
            "0           0          767   33                      NaN   \n",
            "1           1         1080   34                      NaN   \n",
            "2           2         1077   60  Some major design flaws   \n",
            "3           3         1049   50         My favorite buy!   \n",
            "4           4          847   47         Flattering shirt   \n",
            "\n",
            "                                         Review Text  Rating  Recommended IND  \\\n",
            "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
            "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
            "2  I had such high hopes for this dress and reall...       3                0   \n",
            "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
            "4  This shirt is very flattering to all due to th...       5                1   \n",
            "\n",
            "   Positive Feedback Count   Division Name Department Name Class Name  \n",
            "0                        0       Initmates        Intimate  Intimates  \n",
            "1                        4         General         Dresses    Dresses  \n",
            "2                        0         General         Dresses    Dresses  \n",
            "3                        0  General Petite         Bottoms      Pants  \n",
            "4                        6         General            Tops    Blouses  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ0YYIfpYEkI"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 1) **Lowercasing**\n",
        "Lowercasing converts all characters in a text to <blue>**lowercase**</blue>. It ensures uniformity by treating words like <blue>**\"Dog\"**</blue> and <blue>**\"dog\"**</blue> as the same entity. This is important for many NLP tasks since capitalization usually doesn't change the meaning of words.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Natural Language Processing\"\\\n",
        "Output: \"natural language processing\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['Review Text'].astype(str)\n",
        "lowercased_text = text.str.lower()\n",
        "\n",
        "print(lowercased_text.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqnoos5zoqji",
        "outputId": "03c6d7fd-dd23-4202-8fba-56013702612b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    absolutely wonderful - silky and sexy and comf...\n",
            "1    love this dress!  it's sooo pretty.  i happene...\n",
            "2    i had such high hopes for this dress and reall...\n",
            "3    i love, love, love this jumpsuit. it's fun, fl...\n",
            "4    this shirt is very flattering to all due to th...\n",
            "5    i love tracy reese dresses, but this one is no...\n",
            "6    i aded this in my basket at hte last mintue to...\n",
            "7    i ordered this in carbon for store pick up, an...\n",
            "8    i love this dress. i usually get an xs but it ...\n",
            "9    i'm 5\"5' and 125 lbs. i ordered the s petite t...\n",
            "Name: Review Text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stzcyO6UYEkN"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 2) **Removing Punctuation & Special Characters**\n",
        "\n",
        "Punctuation marks (like <blue>**commas**</blue>, <blue>**periods**</blue>, <blue>**dash**</blue> etc.) and special characters (like <blue>**@**</blue>, <blue>**#**</blue>, <blue>**$**</blue>, etc.) are often not meaningful in many NLP tasks. Removing them helps clean the text for better analysis.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Hello! How are you doing @today?\"\\\n",
        "Output: \"Hello How are you doing today\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JWhA4M2qYEkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669a08d5-bc6c-41d5-8034-cd78a41fb974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolutely wonderful  silky and sexy and comfortable\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# text = \"Hello, world ‚úã‚úã! Welcome to?* our&/|~^+%'\\\" NLP - Deep Learningüß† Bootcampü§©.\"\n",
        "text = df['Review Text'].iloc[0]\n",
        "punctuation_pattern = r'[^\\w\\s]'\n",
        "text_cleaned = re.sub(punctuation_pattern, '', text)\n",
        "print(text_cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4as5p2LSYEkP"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 3) **Stop - Words Removal**\n",
        "\n",
        "Stop-words are common words like <blue>**\"the\"**</blue>, <blue>**\"is\"**</blue>, <blue>**\"in\"**</blue>, <blue>**\"and\"**</blue> that don't contribute significant meaning to the text. Removing them helps reduce the size of the dataset <blue>**without losing important context**</blue>.\n",
        "\n",
        "Example:\\\n",
        "Input: \"This is a sample sentence\"\\\n",
        "Output: \"sample sentence\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2mLo9WFYEkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe550e6-5fa5-44d1-c2bb-c7bb0c1c41df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: english\n",
            "Filtered Text: ['Absolutely', 'wonderful', '-', 'silky', 'sexy', 'comfortable']\n",
            "Language: hinglish\n",
            "Filtered Text: ['Absolutely', 'wonderful', '-', 'silky', 'sexy', 'comfortable']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Remove stopwords function for any language\n",
        "def remove_stopwords(text, language):\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    word_tokens = text.split()\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    print(f\"Language: {language}\")\n",
        "    print(\"Filtered Text:\", filtered_text)\n",
        "\n",
        "# English Example\n",
        "en_text = df['Review Text'].iloc[0]\n",
        "remove_stopwords(en_text, \"english\")\n",
        "\n",
        "# Hindi + English - Example\n",
        "hi_text = df['Review Text'].iloc[0]\n",
        "remove_stopwords(hi_text, \"hinglish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQs5uUrSYEkR"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 4) **Removal of URLs**\n",
        "\n",
        "URLs are often <blue>**irrelevant**</blue> in NLP tasks and can add noise to the data. Removing them ensures cleaner text without <blue>**web links**</blue> that don‚Äôt contribute to the context.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Check out this link: https://example.com\"\\\n",
        "Output: \"Check out this link\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UzEwkdU1YEkT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f39d596-2e83-40e8-89aa-8eaec352e4e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Absolutely wonderful - silky and sexy and comfortable'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "text = df['Review Text'].iloc[0]\n",
        "remove_urls(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAANjHxkYEkY"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 5) **Removal of HTML Tags**\n",
        "\n",
        "HTML tags are used in web data but are <blue>**unnecessary**</blue> in NLP tasks. <blue>**Stripping**</blue> out HTML tags cleans the text extracted from <blue>**web pages**</blue>.\n",
        "\n",
        "Example:\\\n",
        "Input: \"&lt;p>This is a paragraph.&lt;/p>\"\\\n",
        "Output: \"This is a paragraph.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ba8_c2c3YEka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe80379-caac-4b78-884e-e0c8cc4720c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Sample after removing HTML tags:\n",
            "0    Absolutely wonderful - silky and sexy and comf...\n",
            "1    Love this dress!  it's sooo pretty.  i happene...\n",
            "2    I had such high hopes for this dress and reall...\n",
            "Name: text_no_html, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# text = \"\"\"<html><div>\n",
        "# <h1>NLP - Deep Learning</h1>\n",
        "# <p>Removal of HTML tags</p>\n",
        "# <a href=\"https://example.com\"></a>\n",
        "# </div></html>\"\"\"\n",
        "\n",
        "# html_tags_pattern = r'<.*?>'\n",
        "# text_without_html_tags = re.sub(html_tags_pattern, '', text)\n",
        "# print(text_without_html_tags)\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    html_tags_pattern = r'<.*?>'\n",
        "    return re.sub(html_tags_pattern, '', text)\n",
        "\n",
        "df['text_no_html'] = df['Review Text'].astype(str).apply(remove_html_tags)\n",
        "print(\"\\n‚úÖ Sample after removing HTML tags:\")\n",
        "print(df['text_no_html'].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKhwe5BiYEkc"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 6) **Stemming**\n",
        "\n",
        "Stemming reduces a word to its <blue>**base**</blue> or <blue>**root**</blue> form, which might not always be a valid word. The idea is to <blue>**strip**</blue> off <blue>**prefixes**</blue> or <blue>**suffixes**</blue>. It‚Äôs a quick and less computationally expensive way of normalizing words. Stemming is preferred when the <blue>**meaning**</blue> of the word is <blue>**not important**</blue> for analysis. for example: <blue>**Spam Detection**</blue>\n",
        "\n",
        "Example:\\\n",
        "Input: \"Playing\", \"Played\", \"Plays\"\\\n",
        "Output: \"Play\"\n",
        "\n",
        "<blue>**Porter stemming**</blue> algorithm is one of the most common stemming algorithms which is basically designed to <blue>**remove**</blue> and <blue>**replace**</blue> well-known <blue>**suffixes**</blue> of English words. Although the Porter Stemming Algorithm was developed for English texts, it can be adapted to different languages. However, it is more effective to use natural language processing tools and algorithms specifically designed for different languages, like the library <blue>**iNLTK**</blue> offers these tools for <blue>**Indic Languages**</blue>. You can find it out here: <blue>**https://github.com/goru001/inltk**</blue>\n",
        "\n",
        "<div style=\"font-style: italic; text-align: center;\" markdown=\"1\">\n",
        "<img width=\"30%\" src=\"https://cdn.botpenguin.com/assets/website/Stemming_53678d43bc.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tCeERUIpYEke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "deefd73f-ba33-41ec-d1b2-39d36f6b9f00"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'text_no_stopwords'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_no_stopwords'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-557556803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_no_stopwords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_no_stopwords'"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    word_tokens = text.split()\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    return stems\n",
        "\n",
        "text =  df['text_no_stopwords']\n",
        "stem_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A3pCt0TYEkf"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 7) **Lemmatization**\n",
        "\n",
        "Lemmatization is a more advanced technique compared to stemming. It <blue>**reduces**</blue> a word to its <blue>**base form (called a lemma)**</blue> while ensuring the <blue>**output**</blue> is a <blue>**valid word**</blue>. It uses context to determine whether the word is in singular, plural, or tense forms.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Running\", \"Ran\"\\\n",
        "Output: \"Run\"\n",
        "\n",
        "In our lemmatization example, we will be using a popular lemmatizer called <blue>**WordNet**</blue> lemmatizer. WordNet is a word association database for English and a useful resource for English lemmatization. A popular lemmatizer used for Hindi is developed by <blue>**JohSnowLabs**</blue> can be found here: <blue>**https://sparknlp.org/2020/07/29/lemma_hi.html**</blue>\n",
        "\n",
        "<div style=\"font-style: italic; text-align: center;\" markdown=\"1\">\n",
        "<img width=\"30%\" src=\"https://cdn.botpenguin.com/assets/website/Lemmatization_5338fc7c3e.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0r0iwa4YEkh"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_word(text):\n",
        "    word_tokens = text.split()\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas\n",
        "\n",
        "text = 'text preprocessing section in course nlp - deep learning'\n",
        "print(lemmatize_word(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT1ANehtYEki"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 8) **Tokenization**\n",
        "\n",
        "Tokenization is the process of <blue>**splitting**</blue> a text into <blue>**individual units**</blue> like words, phrases, or sentences, called <blue>**tokens**</blue>. These tokens form the building blocks for further processing and analysis in NLP tasks.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Congratulations you are almost at the end of this file.\"\\\n",
        "Output: [\"Congratulations\", \"you\", \"are\", \"almost\", \"at\", \"the\", \"end\", \"of\", \"this\", \"file\", \".\"]\n",
        "\n",
        "There are different methods and libraries available to perform tokenization. <blue>**SpaCy**</blue> and <blue>**Gensim**</blue> are some of the libraries that can be used to accomplish the task.\n",
        "Tokenization can be used to separate words or sentences. If the text is split into <blue>**words**</blue> using some separation technique it is called <blue>**word tokenization**</blue> and the same separation done for <blue>**sentences**</blue> is called <blue>**sentence tokenization**</blue>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzsPdvcsYEki",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Congratulations you are almost at the end of this file.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Processing for Natural Language Processing (NLP)**\n",
        "\n",
        "In text processing for NLP, Bag of Words (BoW) is a simple model where a text is represented as a collection of words, disregarding grammar and order, focusing only on word frequency. TF-IDF (Term Frequency-Inverse Document Frequency) improves upon BoW by assigning weights to words based on their importance, emphasizing words that are common in a document but rare across other documents. Word2Vec, a more advanced approach, generates dense word embeddings by training a neural network to map words to vectors, capturing semantic relationships and similarities between words based on context in large datasets. In this tutorial we will be looking at the following topics:\n",
        "1) **Bag of Words (BoW)**\n",
        "2) **Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
        "3) **Encodings**\n",
        "4) **Word Embeddings**\n",
        "5) **Word2Vec**"
      ],
      "metadata": {
        "id": "IqHME5fIaHW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "===================================================================================================="
      ],
      "metadata": {
        "id": "zUfawEqMbp8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 1) **Bag of Words (BoW)**\n",
        "\n",
        "Bag of Words (BoW) is a simple text representation method used in NLP where a document is represented by the <blue>**frequency**</blue> of words in it, <blue>**without considering grammar**</blue>, <blue>**order**</blue>, or <blue>**context**</blue>. It treats every word as <blue>**independent**</blue> and counts its occurrences in the text. For example, in the sentences \"The cat sits on the mat\" and \"The dog sits on the mat\", the BoW would capture the words \"The\", \"cat\", \"sits\", \"on\", \"the\", \"mat\", \"dog\", with their respective frequencies. However, BoW <blue>**cannot capture meaning**</blue> or relationships between words, as it focuses solely on word presence and counts."
      ],
      "metadata": {
        "id": "ez22mVV0btEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Defining all pre-processing steps\n",
        "def preprocessing_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    emoji_pattern = r'^(?:[\\u2700-\\u27bf]|(?:\\ud83c[\\udde6-\\uddff]){1,2}|(?:\\ud83d[\\udc00-\\ude4f]){1,2}|[\\ud800-\\udbff][\\udc00-\\udfff]|[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e]|\\u3299|\\u3297|\\u303d|\\u3030|\\u24c2|\\ud83c[\\udd70-\\udd71]|\\ud83c[\\udd7e-\\udd7f]|\\ud83c\\udd8e|\\ud83c[\\udd91-\\udd9a]|\\ud83c[\\udde6-\\uddff]|\\ud83c[\\ude01-\\ude02]|\\ud83c\\ude1a|\\ud83c\\ude2f|\\ud83c[\\ude32-\\ude3a]|\\ud83c[\\ude50-\\ude51]|\\u203c|\\u2049|\\u25aa|\\u25ab|\\u25b6|\\u25c0|\\u25fb|\\u25fc|\\u25fd|\\u25fe|\\u2600|\\u2601|\\u260e|\\u2611|[^\\u0000-\\u007F])+$'\n",
        "\n",
        "    text = text.split()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
        "    text = ' '.join(text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(emoji_pattern, '', text)\n",
        "    text= re.sub(r'\\s+', ' ', text)\n",
        "    text= text.lower().strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "paragraph = \"\"\"Welcome to the NLP - Deep Learning Bootcamp.\n",
        "            Hope that you have watched all the resources uploaded on our official notion page.\n",
        "            If you have not please begin there, thanks!\n",
        "            \"\"\"\n",
        "\n",
        "sentences_list = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "corpus = [preprocessing_text(sentence) for sentence in sentences_list]\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "paLcyIBocDbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "X_array = X.toarray()\n",
        "\n",
        "print(\"Unique Word List: \\n\", feature_names)\n",
        "print(\"Bag of Words Matrix: \\n\", X_array)"
      ],
      "metadata": {
        "id": "VAAjxcJzcGzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing results in tabular form\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=X_array, columns=feature_names, index=corpus)\n",
        "(df)"
      ],
      "metadata": {
        "id": "AJWJ6AvGcIv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 2) **Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
        "\n",
        "TF-IDF is an extension of BoW that <blue>**assigns**</blue> a <blue>**weight**</blue> to each word, reflecting its <blue>**importance**</blue> in a document relative to a collection of documents (corpus). <blue>**Term Frequency (TF)**</blue> refers to <blue>**how often**</blue> a word appears in a document, while <blue>**Inverse Document Frequency (IDF)**</blue> measures <blue>**how rare**</blue> a word is across the entire corpus. Words that occur frequently in one document but rarely across others get higher TF-IDF scores. For example, <blue>**common words**</blue> like \"the\" and \"is\" receive <blue>**low weights**</blue>, while more <blue>**unique words**</blue> like \"neural\" in an article about AI would have a <blue>**higher weight**</blue>, giving a better sense of relevance."
      ],
      "metadata": {
        "id": "c4IZsAZecK5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define all pre-processing steps except for Lemmatization\n",
        "def preprocessing_text(text):\n",
        "    emoji_pattern = r'^(?:[\\u2700-\\u27bf]|(?:\\ud83c[\\udde6-\\uddff]){1,2}|(?:\\ud83d[\\udc00-\\ude4f]){1,2}|[\\ud800-\\udbff][\\udc00-\\udfff]|[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e]|\\u3299|\\u3297|\\u303d|\\u3030|\\u24c2|\\ud83c[\\udd70-\\udd71]|\\ud83c[\\udd7e-\\udd7f]|\\ud83c\\udd8e|\\ud83c[\\udd91-\\udd9a]|\\ud83c[\\udde6-\\uddff]|\\ud83c[\\ude01-\\ude02]|\\ud83c\\ude1a|\\ud83c\\ude2f|\\ud83c[\\ude32-\\ude3a]|\\ud83c[\\ude50-\\ude51]|\\u203c|\\u2049|\\u25aa|\\u25ab|\\u25b6|\\u25c0|\\u25fb|\\u25fc|\\u25fd|\\u25fe|\\u2600|\\u2601|\\u260e|\\u2611|[^\\u0000-\\u007F])+$'\n",
        "\n",
        "    text= text.lower()\n",
        "    text = text.split()\n",
        "    text = ' '.join(text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(emoji_pattern, '', text)\n",
        "    text= re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "comments = \"\"\"Welcome to the NLP - Deep Learning Bootcamp.\n",
        "            Hope that you have watched all the resources uploaded on our official notion page.\n",
        "            If you have not please begin there, thanks!\n",
        "            \"\"\"\n",
        "\n",
        "sentences_list = nltk.sent_tokenize(comments)\n",
        "corpus = [preprocessing_text(sentence) for sentence in sentences_list]\n",
        "corpus"
      ],
      "metadata": {
        "id": "00yPwWw7cLmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Product of Term Frequency & Inverse Document Frequency\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "V6Pqm5Y1cO5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Word Cloud\n",
        "texts = [\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"NLP is a field of artificial intelligence that focuses on the interaction between computers and human language.\",\n",
        "    \"Sentiment analysis is the process of classifying the emotional intent of text.\",\n",
        "    \"Machine learning is an important application of AI.\",\n",
        "    \"Natural Language Processing is used for text analysis.\",\n",
        "    \"Python libraries like scikit-learn and NLTK are used in NLP.\",\n",
        "    \"AI and machine learning are transforming industries.\",\n",
        "    \"If you are interested in NLP, stay tuned!\"\n",
        "]"
      ],
      "metadata": {
        "id": "L2FwerYScQvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    clean_txt = text.lower()\n",
        "    # Clear characters other than numbers and letters of the alphabet only\n",
        "    clean_txt = re.sub(r'[^0-9a-z√ßƒüƒ±i√∂≈ü√º\\s]', '', clean_txt,\n",
        "                       flags=re.IGNORECASE)\n",
        "\n",
        "    return ' '.join(sorted(clean_txt.split()))\n",
        "\n",
        "cleaned_texts = [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "N-cRGmbPdRfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_texts)"
      ],
      "metadata": {
        "id": "6ajJusPedS6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vectorizer.vocabulary_)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ohZ38-mdUEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advaced Level (Optional for Basic Level)**"
      ],
      "metadata": {
        "id": "VmVo9iL3dVyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 3) **Encodings**\n",
        "\n",
        "<blue>**Label Encoding**</blue> is a technique used in machine learning and data processing to convert <blue>**categorical data**</blue> (such as text-based or symbolic data) into <blue>**numerical values**</blue>. For example, if you have categories like ‚ÄúApple‚Äù, ‚ÄúChicken‚Äù and ‚ÄúBroccoli‚Äù you assign them numerical labels such as 1 for ‚ÄúApple‚Äù, 2 for ‚ÄúChicken‚Äù and 3 for ‚ÄúBroccoli‚Äù.\n",
        "\n",
        "<blue>**One-hot Encoding**</blue> is another technique used in machine learning to convert <blue>**categorical data**</blue> into a format that can be provided to machine learning algorithms more easily. One-hot Encoding takes each categorical value and turns it into a <blue>**binary vector**</blue>. Each category is represented as a binary vector.\n"
      ],
      "metadata": {
        "id": "8c-59GzxdYSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"NLP is a field of artificial intelligence that focuses on the interaction between computers and human language.\",\n",
        "    \"Sentiment analysis is the process of classifying the emotional intent of text.\",\n",
        "    \"Machine learning is an important application of AI.\",\n",
        "    \"Natural Language Processing is used for text analysis.\",\n",
        "    \"Python libraries like scikit-learn and NLTK are used in NLP.\",\n",
        "    \"AI and machine learning are transforming industries.\",\n",
        "    \"If you are interested in NLP, stay tuned!\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus using NLTK\n",
        "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Flatten the list to get all words in the corpus\n",
        "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
        "\n",
        "# Get unique words (vocabulary)\n",
        "vocab = sorted(set(all_words))\n",
        "\n",
        "# Print vocabulary\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "# Reshape the list of words into a 2D array for OneHotEncoder\n",
        "word_array = np.array(all_words).reshape(-1, 1)\n",
        "\n",
        "# Apply OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(word_array)\n",
        "\n",
        "# Print the one-hot encoded data\n",
        "print(\"One-hot encoded matrix:\\n\", one_hot_encoded)\n"
      ],
      "metadata": {
        "id": "b-4qIoB_daUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 4) **Word Embeddings**\n",
        "\n",
        "Word embeddings are <blue>**dense vector representations**</blue> of words that capture their meanings by placing words with <blue>**similar meanings closer**</blue> in <blue>**vector space**</blue>. Unlike BoW or one-hot encoding, embeddings <blue>**capture relationships**</blue> between words based on context. For example, in an embedding space, words like \"king\" and \"queen\" or \"apple\" and \"fruit\" would be closer together, reflecting their <blue>**semantic similarity**</blue>. These embeddings are learned from large datasets and can be used as inputs to machine learning models for various NLP tasks. Popular techniques for generating word embeddings include <blue>**Word2Vec**</blue>, <blue>**GloVe**</blue>, and <blue>**FastText**</blue>."
      ],
      "metadata": {
        "id": "XPA4J8ZKdel9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 5) **Word2Vec**\n",
        "\n",
        "Word2Vec is a model used to <blue>**learn word embeddings**</blue>, developed by <blue>**Google**</blue>. It transforms words into <blue>**continuous vector representations**</blue> using neural networks. There are two primary approaches: <blue>**Skip-gram**</blue> and <blue>**Continuous Bag of Words (CBOW)**</blue>. Skip-gram <blue>**predicts surrounding words**</blue> given a <blue>**target word**</blue>, while CBOW <blue>**predicts**</blue> the <blue>**target word**</blue> based on its <blue>**context**</blue>. For example, in the sentence \"The dog barked loudly\", Skip-gram might learn that \"dog\" is likely to appear near words like \"barked\", while CBOW learns that \"dog\" fits between \"The\" and \"barked\". This method helps capture semantic relationships between words in a computationally efficient manner.\n",
        "\n"
      ],
      "metadata": {
        "id": "x_SAwrsCdhL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [\n",
        "    ['this', 'is', 'an', 'example', 'sentence', 'for', 'word2vec'],\n",
        "    ['we', 'are', 'creating', 'a', 'word2vec', 'model', 'using', 'the', 'gensim', 'library'],\n",
        "    ['we', 'are', 'working', 'with', 'cbow', 'and', 'skipgram', 'models'],\n",
        "    ['python', 'is', 'a', 'programming', 'language', 'for', 'natural', 'language', 'processing'],\n",
        "    ['word2vec', 'is', 'one', 'of', 'the', 'word', 'embedding', 'techniques'],\n",
        "    ['the', 'word2vec', 'model', 'is', 'used', 'for', 'word', 'embeddings'],\n",
        "    ['gensim', 'provides', 'an', 'easy', 'way', 'to', 'train', 'word2vec', 'models'],\n",
        "    ['cbow', 'and', 'skipgram', 'are', 'two', 'types', 'of', 'word2vec', 'models'],\n",
        "    ['word2vec', 'is', 'a', 'technique', 'for', 'natural', 'language', 'processing'],\n",
        "    ['this', 'sentence', 'is', 'about', 'word2vec', 'and', 'its', 'applications'],\n",
        "    ['word2vec', 'is', 'a', 'popular', 'word', 'embedding', 'method'],\n",
        "    ['many', 'researchers', 'use', 'word2vec', 'for', 'various', 'nlp', 'tasks'],\n",
        "    ['the', 'skipgram', 'model', 'focuses', 'on', 'predicting', 'context', 'words'],\n",
        "    ['cbow', 'model', 'predicts', 'the', 'center', 'word', 'from', 'context', 'words'],\n",
        "    ['natural', 'language', 'processing', 'involves', 'working', 'with', 'large', 'datasets']\n",
        "]\n",
        "\n",
        "cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0, alpha=0.03, min_alpha=0.0007, epochs=100)\n",
        "skipgram_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1, alpha=0.03, min_alpha=0.0007, epochs=100)\n",
        "\n",
        "cbow_model.train(sentences, total_examples=len(sentences), epochs=100)\n",
        "skipgram_model.train(sentences, total_examples=len(sentences), epochs=100)\n",
        "\n",
        "word_vectors_cbow = cbow_model.wv\n",
        "similarity_cbow = word_vectors_cbow.similarity('word2vec', 'gensim')\n",
        "print(f\"Similarity between 'word2vec' and 'gensim': {similarity_cbow} with CBOW\")\n",
        "\n",
        "\n",
        "word_vectors_skipgram= skipgram_model.wv\n",
        "similarity_skip = word_vectors_skipgram.similarity('word2vec', 'gensim')\n",
        "print(f\"Similarity between 'word2vec' and 'gensim': {similarity_skip} with Skip-Gram\")"
      ],
      "metadata": {
        "id": "7Z8EYtd4djlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}