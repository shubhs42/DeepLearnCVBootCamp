{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhs42/DeepLearnCVBootCamp/blob/main/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBKn00GGP-ik"
      },
      "source": [
        "# **Assignment 2**\n",
        "## **Artificial Neural Network for Text Sentiment Classification**\n",
        "\n",
        "Text sentiment classification using neural networks (NN) involves training a model to analyze and classify the sentiment expressed in text, such as positive, negative, or neutral. Neural networks, particularly recurrent networks (RNNs) or transformers (which will be explored subsequently), can capture the sequential nature of text data, making them well-suited for this task. By embedding words into dense vectors, a feed forward neural network can learn to associate patterns in word usage with sentiment labels. During training, the model adjusts its weights to minimize error, enabling it to generalize to unseen text, ultimately predicting sentiment with high accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg5A1A2YP-im"
      },
      "source": [
        "===================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8gsjW74P-in"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 1**\n",
        "This code imports necessary libraries for text preprocessing, model building, and evaluation.\n",
        "1)  It uses <blue>**pandas**</blue> for <green>**data handling**</green>.\n",
        "2) <blue>**Matplotlib**</blue> for <green>**plotting**</green>.\n",
        "3) <blue>**NLTK**</blue> for <green>**tokenizing**</green> and <green>**stemming**</green> text.\n",
        "4) It includes <blue>**scikit-learn**</blue> for <green>**splitting data**</green> and <green>**evaluation (classification_report)**</green>.\n",
        "5) <blue>**Torch**</blue> is used to <green>**define**</green>, <green>**train**</green>, and <green>**optimize**</green> neural networks.\n",
        "6) <blue>**gensim**</blue> is used for <green>**dictionary creation**</green> and <green>**token mapping**</green>, helping to convert text into numerical format for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsu0AUg8P-in"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from gensim import corpora\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCWSFo-bP-io"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 2**\n",
        "This code <green>**loads**</green> the <blue>**\"yelp_reviews_subset_2.csv\"**</blue> dataset downloaded from the Notion page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4xgi7ZNP-io"
      },
      "outputs": [],
      "source": [
        "# TODO: Download and Load the \"yelp_reviews_subset_2.csv\" dataset uploaded on the notion page using pandas\n",
        "df = None # Replace None with the correct code\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2hQ8kRuP-io"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 3**\n",
        "This code maps review star ratings to sentiment categories and visualizes the sentiment distribution.\n",
        "\n",
        "1) The function <blue>**map_sentiment**</blue> converts star ratings into <green>**three categories**</green>: negative (-1 for stars ≤ 2), neutral (0 for 3 stars), and positive (1 for stars ≥ 4).\n",
        "2) It applies this function to the <blue>**Labels**</blue> column in df, creating a new column <blue>**sentiment**</blue>.\n",
        "3) The code then plots a bar chart showing the <green>**distribution of the sentiments**</green> using matplotlib and pandas, labeling the x-axis as \"Sentiment\" and the y-axis as \"No. of rows in df\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93d-J5GeP-ip"
      },
      "outputs": [],
      "source": [
        "def map_sentiment(stars_received):\n",
        "    if stars_received <= 2:\n",
        "        return -1\n",
        "    elif stars_received == 3:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# Mapping stars to sentiment into three categories\n",
        "df['sentiment'] = [ map_sentiment(x) for x in df['Labels']]\n",
        "# Plotting the sentiment distribution\n",
        "plt.figure()\n",
        "pd.value_counts(df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"No. of rows in df\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JnekDAKP-ip"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 4**\n",
        "This code tokenizes text and stems the words using NLTK tools.\n",
        "\n",
        "1) It <green>**tokenizes**</green> each sentence in the dataframe <blue>**df['tokenized_text']**</blue> using <blue>**word_tokenize**</blue> from NLTK, applying <green>**list comprehension**</green> to store the tokens as a list.\n",
        "2) Next, it uses the <blue>**Porter Stemmer**</blue> to <green>**stem**</green> each token in <blue>**df['tokenized_text']**</blue>, storing the stemmed words in <blue>**df['stemmed_tokens']**</blue> using <green>**list comprehension**</green>.\n",
        "\n",
        "Both processes are applied to make the text more suitable for sentiment classification by reducing it to basic word forms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHPkH5WEP-iq"
      },
      "outputs": [],
      "source": [
        "# TODO: Tokenize each sentence into word tokens and store them as a list in the dataframe\n",
        "# Use List Comprehension\n",
        "df['tokenized_text'] = None # Replace None with the correct code\n",
        "print(df['tokenized_text'].head(10))\n",
        "\n",
        "ps = PorterStemmer()\n",
        "# TODO: Use the Porter Stemmer to find stem words of each word for all the words in df[\"tokenized_text\"]\n",
        "# Hint: Use List Comprehension\n",
        "df['stemmed_tokens'] = None # Replace None with the correct code\n",
        "df['stemmed_tokens'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRWJ81tyP-iq"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 5**\n",
        "This code splits the dataset into training and testing sets for model evaluation.\n",
        "<blue>**split_train_test**</blue> function uses <blue>**train_test_split**</blue> from scikit-learn to divide the dataset into <green>**training (70%)**</green> and <green>**testing (30%)**</green> sets based on the tokenized_text, stemmed_tokens, and other columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_NPDenGP-iq"
      },
      "outputs": [],
      "source": [
        "# Train Test Split Function\n",
        "def split_train_test(df, test_size=0.3, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(df[[\"Text\", \"Labels\", \"tokenized_text\", \"stemmed_tokens\"]],\n",
        "                                                        df['sentiment'],\n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size,\n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGRjZU7XP-ir"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 6**\n",
        "This code sets the <blue>**device**</blue> for running the model, either using <green>**GPU**</green> if available or falling back to <green>**CPU**</green>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7kZXoAlP-ir"
      },
      "outputs": [],
      "source": [
        "# TODO: Set device (GPU if available, else CPU)\n",
        "device = None # Replace None with the correct code\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2835pU8P-ir"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 7**\n",
        "This code defines a Feedforward Neural Network model with three fully connected layers using PyTorch.\n",
        "\n",
        "1) <blue>**fc1**</blue>: First fully connected layer that takes the <green>**input dimension (input_dim)**</green> and maps it to a <green>**hidden dimension (hidden_dim)**</green>.\n",
        "2) <blue>**fc2**</blue>: Second fully connected layer that maps the <green>**hidden dimension (hidden_dim)**</green> to another <green>**hidden dimension (hidden_dim)**</green>.\n",
        "3) <blue>**fc3**</blue>: Final layer that maps the <green>**hidden dimension**</green> to the <green>**output dimension (output_dim)**</green>, which corresponds to the sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsMNtsFjP-ir"
      },
      "outputs": [],
      "source": [
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "\n",
        "        # TODO: Add an fc1 layer as described below\n",
        "        # Linear function 1: vocab_size --> hiiden_dim\n",
        "        self.fc1 = None # Replace None with the correct code\n",
        "\n",
        "        # Non-linearity 1\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # TODO: Add an fc2 layer as described below\n",
        "        # Linear function 2: hidden_dim --> hidden_dim\n",
        "        self.fc2 = None # Replace None with the correct code\n",
        "\n",
        "        # Non-linearity 2\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # TODO: Add an fc3 layer as described below\n",
        "        # Linear function 3 (readout): hidden_dim --> output_dim\n",
        "        self.fc3 = None  # Replace None with the correct code\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO: Forward pass through fully connected layer 1\n",
        "        out = None # Replace None with the correct code\n",
        "\n",
        "        # Non-linearity 1\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        # TODO: Forward pass through fully connected layer 2\n",
        "        out = None # Replace None with the correct code\n",
        "\n",
        "        # Non-linearity 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # TODO: Forward pass through fully connected layer 3\n",
        "        out = None # Replace None with the correct code\n",
        "\n",
        "        return F.softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsTy4gvWP-ir"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 8**\n",
        "This code defines a function to create a dictionary of tokens from a DataFrame using Gensim's corpora.Dictionary.\n",
        "\n",
        "1) <blue>**make_dict**</blue> generates a <green>**token dictionary**</green> from the <blue>**stemmed_tokens**</blue> column in the DataFrame.\n",
        "2) If <blue>**padding=True**</blue>, it adds a <green>**padding token ('pad')**</green> to the dictionary, which is useful for models that require fixed input sizes.\n",
        "3) If <blue>**padding=False**</blue>, the dictionary is created directly from the tokenized text <green>**without adding a padding token**</green>.\n",
        "4) The <blue>**review_dict**</blue> variable stores the dictionary, and in this case, it is created <green>**without padding**</green> by calling the function with <blue>**padding=False**</blue>.\n",
        "\n",
        "This process is helpful for converting text tokens into numerical indices that can be used as input for machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-JTlz0tP-is"
      },
      "outputs": [],
      "source": [
        "# Function to return the dictionary either with padding word or without padding\n",
        "def make_dict(top_data_df_small, padding=True):\n",
        "    if padding:\n",
        "        print(\"Dictionary with padded token added\")\n",
        "        review_dict = corpora.Dictionary([['pad']])\n",
        "        review_dict.add_documents(top_data_df_small['stemmed_tokens'])\n",
        "    else:\n",
        "        print(\"Dictionary without padding\")\n",
        "        review_dict = corpora.Dictionary(top_data_df_small['stemmed_tokens'])\n",
        "    return review_dict\n",
        "\n",
        "# Make the dictionary without padding for the basic models\n",
        "review_dict = make_dict(df, padding=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbKQBL6mP-is"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 9**\n",
        "This code creates functions to generate input and target tensors for the neural network.\n",
        "\n",
        "1) <blue>**make_bow_vector**</blue>: This function creates a <green>**bag-of-words (BOW)**</green> vector from a tokenized sentence.\n",
        "\n",
        "2) It initializes a <blue>**zero vector**</blue> of size <blue>**VOCAB_SIZE (30,056)**</blue>, where <green>**each index**</green> represents a word from the vocabulary.\n",
        "\n",
        "3) <blue>**make_target**</blue>: This function converts a <green>**sentiment label (-1, 0, or 1)**</green> into a tensor for the output.\n",
        "\n",
        "4) <blue>**Negative sentiment (-1)**</blue> maps to <green>**0**</green>, <blue>**neutral sentiment (0)**</blue> maps to <green>**1**</green>, and <blue>**positive sentiment (1)**</blue> maps to <green>**2**</green>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzRayQh_P-is"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 30056\n",
        "NUM_LABELS = 3\n",
        "\n",
        "# Function to make bow vector to be used as input to network\n",
        "def make_bow_vector(review_dict, sentence):\n",
        "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)\n",
        "    for word in sentence:\n",
        "        vec[review_dict.token2id[word]] += 1\n",
        "    return vec.view(1, -1).float()\n",
        "\n",
        "# Function to get the output tensor\n",
        "def make_target(label):\n",
        "    if label == -1:\n",
        "        return torch.tensor([0], dtype=torch.long, device=device)\n",
        "    elif label == 0:\n",
        "        return torch.tensor([1], dtype=torch.long, device=device)\n",
        "    else:\n",
        "        return torch.tensor([2], dtype=torch.long, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_OU0gZdP-it"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 10**\n",
        "1) <blue>**Model Initialization**</blue>: The <green>**Feedforward Neural Network**</green> is instantiated with input_dim, hidden_dim, and output_dim.\n",
        "2) <blue>**Device Transfer**</blue>: The model is moved to the <green>**GPU**</green> or <green>**CPU**</green> as specified earlier.\n",
        "3) <blue>**Loss Function**</blue>: The loss function is set to <green>**CrossEntropyLoss**</green>, suitable for multi-class classification.\n",
        "4) <blue>**Optimizer**</blue>: The optimizer is <green>**SGD (Stochastic Gradient Descent)**</green> with a <green>**learning rate**</green> of <green>**1e-3**</green>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERQt5edLP-it"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(review_dict)\n",
        "\n",
        "input_dim = VOCAB_SIZE\n",
        "hidden_dim = 500\n",
        "output_dim = 3\n",
        "num_epochs = 100\n",
        "\n",
        "# TODO: Call the FeedForwardNeuralNetwork Class object with appropriate arguements\n",
        "ff_nn_bow_model = None # Replace None with the correct code\n",
        "\n",
        "# TODO: Move the model to device\n",
        "ff_nn_bow_model = None # Replace None with the correct code\n",
        "\n",
        "# TODO: Define the loss function as Cross Entropy Loss\n",
        "loss_function = None # Replace None with the correct code\n",
        "\n",
        "# TODO: Define a Stochastic Gradient Descent Optimizer with learning rate of 1e-3\n",
        "optimizer = None # Replace None with the correct code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9rIhSF0P-it"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 11**\n",
        "1) <blue>**Bag-of-Words Vector**</blue>: The <green>**make_bow_vector**</green> function is used to create a BOW vector from the stemmed tokens.\n",
        "2) <blue>**Forward Pass**</blue>: The <green>**BOW vector**</green> is passed through the <green>**feedforward neural network**</green> to get the output probabilities.\n",
        "3) <blue>**Loss Calculation**</blue>: The <green>**CrossEntropyLoss**</green> is computed between the output probabilities and the target label.\n",
        "4) <blue>**Gradient Update**</blue>: <green>**Gradients**</green> are calculated and the parameters are updated using backpropagation and <green>**optimizer.step()**</green>.\n",
        "5) <blue>**Loss Logging**</blue>: The <green>**average loss per epoch**</green> is written to the loss file for tracking training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GvaOfT_P-it"
      },
      "outputs": [],
      "source": [
        "# Open the file for writing loss\n",
        "ffnn_loss_file_name = 'ffnn_bow_class_big_loss_500_epoch_100_less_lr.csv'\n",
        "f = open(ffnn_loss_file_name,'w')\n",
        "f.write('iter, loss')\n",
        "f.write('\\n')\n",
        "losses = []\n",
        "iter = 0\n",
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch+1) % 25 == 0:\n",
        "        print(\"Epoch completed: \" + str(epoch+1))\n",
        "    train_loss = 0\n",
        "    for index, row in X_train.iterrows():\n",
        "        # Clearing the accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: Make the bag of words vector for stemmed tokens\n",
        "        bow_vec = None # Replace None with the correct code\n",
        "\n",
        "        # TODO: Forward pass to get output\n",
        "        probs = None # Replace None with the correct code\n",
        "\n",
        "        # Get the target label\n",
        "        target = make_target(Y_train['sentiment'][index])\n",
        "\n",
        "        # TODO: Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = None # Replace None with the correct code\n",
        "\n",
        "        # Accumulating the loss over time\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n",
        "    f.write('\\n')\n",
        "    train_loss = 0\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWZfg2QgP-it"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 12**\n",
        "This code evaluates the performance of the trained feedforward neural network on the test dataset and visualizes the loss during training.\n",
        "\n",
        "1) <blue>**Prediction Loop**</blue>: It iterates through the test set X_test without tracking gradients <green>**(using torch.no_grad())**</green> for efficient inference. For each test sample, it generates a <green>**BOW vector**</green> and passes it through the model to obtain predicted probabilities.\n",
        "\n",
        "2) <blue>**Classification Report**</blue>: The <green>**classification_report**</green> from <green>**sklearn.metrics**</green> is printed to evaluate the model's performance, displaying precision, recall, F1-score, and support for each class.\n",
        "\n",
        "3) <blue>**Loss DataFrame**</blue>: The loss data saved in <green>**ffnn_loss_file_name**</green> is read into a DataFrame.\n",
        "\n",
        "4) <blue>**Loss Plotting**</blue>: The loss is plotted using pandas and saved as a JPEG file named <green>**\"ffnn_bow_loss_500_padding_100_epochs_less_lr.jpg\"**</green>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOjQ9wo2P-iu"
      },
      "outputs": [],
      "source": [
        "bow_ff_nn_predictions = []\n",
        "original_lables_ff_bow = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for index, row in X_test.iterrows():\n",
        "        bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])\n",
        "        probs = ff_nn_bow_model(bow_vec)\n",
        "        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
        "        original_lables_ff_bow.append(make_target(Y_test['sentiment'][index]).cpu().numpy()[0])\n",
        "\n",
        "print(classification_report(original_lables_ff_bow,bow_ff_nn_predictions))\n",
        "ffnn_loss_df = pd.read_csv(ffnn_loss_file_name)\n",
        "print(len(ffnn_loss_df))\n",
        "print(ffnn_loss_df.columns)\n",
        "ffnn_plt_500_padding_100_epochs = ffnn_loss_df[' loss'].plot()\n",
        "fig = ffnn_plt_500_padding_100_epochs.get_figure()\n",
        "fig.savefig(\"ffnn_bow_loss_500_padding_100_epochs_less_lr.jpg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Steps (Optional for people targeting basic level)"
      ],
      "metadata": {
        "id": "IaU71TxWRSUS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7151e5f5"
      },
      "source": [
        "### **Step - 13**\n",
        "This code defines and trains a Feedforward Neural Network model with Dropout for regularization.\n",
        "\n",
        "1. **Model Initialization**: The **Feedforward Neural Network** is instantiated with input_dim, hidden_dim, output_dim, and a dropout rate.\n",
        "2. **Device Transfer**: The model is moved to the **GPU** or **CPU**.\n",
        "3. **Loss Function**: The loss function is set to **CrossEntropyLoss**.\n",
        "4. **Optimizer**: The optimizer is **SGD (Stochastic Gradient Descent)** with a **learning rate** of **1e-3**.\n",
        "5. **Training Loop**: The model is trained on the training data, calculating and minimizing the loss using backpropagation and the optimizer.\n",
        "6. **Loss Logging**: The average loss per epoch is written to a new loss file for tracking training progress."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Feedforward Neural Network model with dropout\n",
        "# TODO: Create a model instance with the given input, hidden, and output dimensions\n",
        "ff_nn_bow_model_dropout = None  # Replace None with model initialization (include dropout_rate parameter)\n",
        "ff_nn_bow_model_dropout.to(device)\n",
        "\n",
        "# TODO: Define loss function and optimizer\n",
        "loss_function_dropout = None  # Replace None with appropriate loss function\n",
        "optimizer_dropout = None  # Replace None with optimizer\n",
        "\n",
        "# TODO: Prepare to record training loss\n",
        "ffnn_loss_file_name_dropout = 'ffnn_bow_class_big_loss_500_epoch_100_less_lr_dropout.csv'\n",
        "f_dropout = open(ffnn_loss_file_name_dropout, 'w')\n",
        "f_dropout.write('iter, loss\\n')\n",
        "losses_dropout = []\n",
        "iter_dropout = 0\n",
        "\n",
        "# Train the model\n",
        "ff_nn_bow_model_dropout.train()\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch + 1) % 25 == 0:\n",
        "        print(f\"Epoch completed: {epoch + 1}\")\n",
        "\n",
        "    train_loss_dropout = 0\n",
        "\n",
        "    # TODO: Iterate through training samples\n",
        "    for index, row in X_train.iterrows():\n",
        "        optimizer_dropout.zero_grad()\n",
        "\n",
        "        # TODO: Create bag-of-words input vector and target\n",
        "        bow_vec = None  # Replace None with code to create BoW vector from review_dict and tokens (call previous functions defined)\n",
        "        probs = None  # Replace None with forward pass through the model\n",
        "        target = None  # Replace None with code to create target tensor\n",
        "\n",
        "        # TODO: Compute loss\n",
        "        loss = None  # Replace None with loss computation\n",
        "        train_loss_dropout += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer_dropout.step()\n",
        "\n",
        "    # Log average loss for the epoch\n",
        "    f_dropout.write(f\"{epoch + 1},{train_loss_dropout / len(X_train)}\\n\")\n",
        "    train_loss_dropout = 0\n",
        "\n",
        "f_dropout.close()\n"
      ],
      "metadata": {
        "id": "rIDC4ZDxRhm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc14a55e"
      },
      "source": [
        "### **Step - 14**\n",
        "This code evaluates the performance of the trained feedforward neural network with Dropout on the test dataset and visualizes the loss during training, comparing it with the model without dropout.\n",
        "\n",
        "1. **Prediction Loop**: It iterates through the test set X_test with the dropout model.\n",
        "2. **Classification Report**: The **classification_report** from **sklearn.metrics** is printed to evaluate the model's performance with dropout.\n",
        "3. **Loss DataFrame**: The loss data saved for both models is read into DataFrames.\n",
        "4. **Loss Plotting**: The loss for both models is plotted on the same graph for comparison and saved as a JPEG file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5cc5d1"
      },
      "source": [
        "bow_ff_nn_predictions_dropout = []\n",
        "original_lables_ff_bow_dropout = []\n",
        "\n",
        "ff_nn_bow_model_dropout.eval() # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for index, row in X_test.iterrows():\n",
        "        bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])\n",
        "        probs = ff_nn_bow_model_dropout(bow_vec)\n",
        "        bow_ff_nn_predictions_dropout.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
        "        original_lables_ff_bow_dropout.append(make_target(Y_test['sentiment'][index]).cpu().numpy()[0])\n",
        "\n",
        "print(\"Classification Report with Dropout:\")\n",
        "print(classification_report(original_lables_ff_bow_dropout, bow_ff_nn_predictions_dropout))\n",
        "\n",
        "ffnn_loss_df = pd.read_csv(ffnn_loss_file_name)\n",
        "ffnn_loss_df_dropout = pd.read_csv(ffnn_loss_file_name_dropout)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ffnn_loss_df['iter'], ffnn_loss_df[' loss'], label='Without Dropout')\n",
        "plt.plot(ffnn_loss_df_dropout['iter'], ffnn_loss_df_dropout[' loss'], label='With Dropout')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss with and Without Dropout')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"ffnn_bow_loss_comparison_dropout.jpg\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}